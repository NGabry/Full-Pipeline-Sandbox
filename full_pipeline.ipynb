{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3625196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import biom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from qiime2 import Artifact, Metadata\n",
    "from qiime2.plugins import cutadapt as cut\n",
    "from qiime2.plugins.demux.visualizers import summarize\n",
    "from qiime2.plugins.dada2.methods import denoise_single\n",
    "from qiime2.plugins.feature_table.methods import filter_seqs\n",
    "from qiime2.plugins.feature_table.methods import filter_features\n",
    "from qiime2.plugins.feature_table.visualizers import tabulate_seqs\n",
    "from qiime2.plugins.feature_table.visualizers import summarize as summarize_table\n",
    "\n",
    "from remultiplexing import remultiplex\n",
    "from index_jump import calculate_IJR\n",
    "from index_jump import recalculate_IJR\n",
    "from per_sample_filtering import per_sample_filter\n",
    "from length_filtering import length_filter\n",
    "from qiime2.plugins.feature_classifier.pipelines import classify_hybrid_vsearch_sklearn\n",
    "from qiime2.plugins.feature_classifier.methods import classify_consensus_vsearch, classify_sklearn\n",
    "from qiime2.plugins.taxa.visualizers import barplot\n",
    "from qiime2.plugins.phylogeny.pipelines import align_to_tree_mafft_fasttree\n",
    "from qiime2.plugins.diversity.pipelines import core_metrics_phylogenetic\n",
    "\n",
    "def extract_tsv(file, dest):\n",
    "    with tempfile.TemporaryDirectory() as temp:\n",
    "        file.export_data(temp)\n",
    "        temp_pathlib = pathlib.Path(temp)\n",
    "        for file in temp_pathlib.iterdir():\n",
    "            if file.suffix == '.tsv':\n",
    "                shutil.copy(file, dest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remultiplex(name of bam file)\n",
    "remultiplex('subsample.bam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df30bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gzip the all_seqs.fastq file produced during remultiplexing \n",
    "os.system('gzip all_seqs.fastq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9692ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in sample map\n",
    "sample_map = 'Sample_Map_Full.txt'\n",
    "\n",
    "\n",
    "metadata = Metadata.load(sample_map)\n",
    "metadata_df = pd.read_csv(sample_map, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a gzipped fastq as a fully multiplexed file into qiime2 \n",
    "mux = Artifact.import_data(\"MultiplexedSingleEndBarcodeInSequence\", \"all_seqs.fastq.gz\")\n",
    "\n",
    "#Saving as a qza\n",
    "mux.save('mux.qza')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d690791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demultiplexing the reads according to their barcodes \n",
    "demux, untrimmed = cut.methods.demux_single(mux, \n",
    "                                            metadata.get_column('BarcodeSequence'), \n",
    "                                            error_rate = 0)\n",
    "demux.save('demux.qza')\n",
    "d = summarize(demux)\n",
    "d.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trimming ANML primers off the demultiplexed reads\n",
    "ANML_primers = [\"^GGTCAACAAATCATAAAGATATTGG...GGATTTGGAAATTGATTAGTWCCATC\"]\n",
    "\n",
    "trimmed_demux = cut.methods.trim_single(demux, \n",
    "                                        cores=16, \n",
    "                                        adapter = ANML_primers, \n",
    "                                        indels = True,\n",
    "                                        minimum_length = 170, \n",
    "                                        discard_untrimmed = True)\n",
    "trimmed_demux = trimmed_demux.trimmed_sequences\n",
    "trimmed_demux.save('trimmed_demux.qza')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d33b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Saved Files\n",
    "# trimmed_demux = Artifact.load('trimmed_demux.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10c4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of false reads expected in a single sample: 1.519759980930058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: tsvs: File exists\n"
     ]
    }
   ],
   "source": [
    "#Calculate max Index Jump Rate using calculate_IJR\n",
    "\n",
    "max_IJR = calculate_IJR(trimmed_demux, metadata, metadata_df)\n",
    "max_IJR = round(max_IJR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DADA2\n",
    "table, rep_seqs, stats = denoise_single(trimmed_demux,\n",
    "                                        trunc_len = 0, \n",
    "                                        n_threads = 0)\n",
    "rep_seqs.save('rep_seqs.qza')\n",
    "table.save('table.qza')\n",
    "stats.save('stats.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53f94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-loading artifacts\n",
    "rep_seqs = Artifact.load('rep_seqs.qza')\n",
    "table = Artifact.load('table.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851ddb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of false reads expected in a single sample: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: csvs: File exists\n"
     ]
    }
   ],
   "source": [
    "#RECALCULATE_IJR using recalculate_IJR??? \n",
    "re_max_IJR = recalculate_IJR(rep_seqs, table, metadata_df)\n",
    "re_max_IJR = round(re_max_IJR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "574a3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: fastas: File exists\n"
     ]
    }
   ],
   "source": [
    "#Filtering by Length\n",
    "rep_seqs_filt_v1, table_filt_v1 = length_filter(rep_seqs, table, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc1c5b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'table' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/86h8nww96zg4j028mg469_bh0000gn/T/ipykernel_50614/853821201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Filtering by Frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtable_filt_v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_sample_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_max_IJR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_filt_v1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Coding/Full-Pipeline-Sandbox/per_sample_filtering.py\u001b[0m in \u001b[0;36mper_sample_filter\u001b[0;34m(filtering_integer, feature_table)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#Convert table to df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mbiom_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mbiom_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'table' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Filtering by Frequency \n",
    "table_filt_v2 = per_sample_filter(re_max_IJR, table_filt_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Taxonomy Assignment\n",
    "# vsearch_taxonomy =      classify_consensus_vsearch(query = rep_seqs_filt_v1,\n",
    "#                                            reference_reads = NoAm_seqs,\n",
    "#                                            reference_taxonomy = NoAm_tax,\n",
    "#                                            maxaccepts = 1,\n",
    "#                                            perc_identity = .99,\n",
    "#                                            query_cov = .99,\n",
    "#                                            strand = 'both',\n",
    "#                                            threads = 16)\n",
    "# #Save taxonomy\n",
    "# vsearch_taxonomy = vsearch_taxonomy.classification\n",
    "# vsearch_taxonomy.save('vsearch_taxonomy.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Grab the list of Feature ID's which were not assigned with Vsearch \n",
    "# os.system('mkdir tsvs')\n",
    "# extract_tsv(vsearch_taxonomy, 'tsvs')\n",
    "# vsearch_df = pd.read_csv('tsvs/taxonomy.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Selecting only the features which were assigned\n",
    "# #Then sending them to a CSV which will make up our exclusion metadata\n",
    "\n",
    "# features_to_exclude = vsearch_df[vsearch_df['Taxon'] != 'Unassigned']\n",
    "\n",
    "# features_to_exclude['Feature ID'].to_csv('Features-to-exclude.csv', index=False)\n",
    "\n",
    "# exclude = Metadata.load(\"Features-to-exclude.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7115111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Filter merged_seqs based on seqs_to_exclude\n",
    "# unassigned_seqs  = filter_seqs(rep_seqs_filt_v1, \n",
    "#                                metadata = exclude,\n",
    "#                                exclude_ids = True)\n",
    "\n",
    "# unassigned_table  = filter_seqs(table_filt_v2, \n",
    "#                                metadata = exclude,\n",
    "#                                exclude_ids = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Naive Bayes Taxonomy Assignment \n",
    "\n",
    "# sklearn_taxonomy = classify_sklearn(reads = unassigned_seqs.filtered_data,\n",
    "#                                     classifier = NoAm_class,\n",
    "#                                     n_jobs = -2,\n",
    "#                                     read_orientation = 'auto')\n",
    "\n",
    "# #Then merge taxonomies\n",
    "# os.system('mkdir tsvs_sklearn')\n",
    "# extract_tsv(taxonomy, 'tsvs_sklearn')\n",
    "\n",
    "\n",
    "# sklearn_df = pd.read_csv('tsvs_sklearn/taxonomy.tsv', sep = '\\t')\n",
    "\n",
    "# vsearch_df = features_to_exclude\n",
    "\n",
    "# frames = [vsearch_df, sklearn,df]\n",
    "\n",
    "# merged_taxonomies = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "# merged_taxonomies = merged_taxonomies.set_index('Feature ID')\n",
    " \n",
    "# merged_taxonomy = Artifact.import_data(\"FeatureData[Taxonomy]\", merged_taxonomies)\n",
    "\n",
    "# merged_taxonomy.save('merged_taxonomy.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d76b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot Creation\n",
    "# taxonomy = Artifact.load('merged_taxonomy.qza')\n",
    "# barplot = barplot(table_filt_v2, merged_taxonomy, metadata)\n",
    "# barplot = barplot.visualization\n",
    "# barplot.save('merged_barplot.qzv')\n",
    "# barplot = Visualization.load('merged_barplot.qzv')\n",
    "# barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06506359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Filter out low Zono, P, and C level IDs from table and seqs\n",
    "# filter_list = [\"k__Animalia;p__Chordata;c__Aves;o__Passeriformes;f__Passerellidae;g__Zonotrichia;s__albicollis\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__;o__;f__;g__;s__\",\n",
    "#                \"k__Animalia;p__Arthropoda;__;__;__;__;__\",\n",
    "#                \"k__Animalia;p__Arthropoda\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Insecta;o__;f__;g__;s__\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Insecta;__;__;__;__\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Insecta\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Arachnida;o__;f__;g__;s__\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Arachnida;__;__;__;__\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Arachnida\",\n",
    "#                \"k__Animalia;p__Arthropoda;c__Collembola\"]\n",
    "\n",
    "# for i in filter_list:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
